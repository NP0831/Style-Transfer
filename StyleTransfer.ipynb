{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nXAHG0-fyCSj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms as T\n",
        "from torchvision.transforms import ToPILImage\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "xVI8QZhLyInb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageLoader:\n",
        "    def __init__(self, size: (int, tuple), resize: bool = True, interpolation=2):\n",
        "        transforms = []\n",
        "        if resize:\n",
        "            transforms.append(T.Resize(size=size, interpolation=interpolation))\n",
        "        transforms.append(T.ToTensor())\n",
        "        self.transforms = T.Compose(transforms)\n",
        "\n",
        "    def read_image(self, filepath: str) -> torch.Tensor:\n",
        "        image = Image.open(filepath)\n",
        "        image = self.transforms(image)\n",
        "        image = image.to(device, torch.float)\n",
        "        return image\n",
        "\n",
        "    @staticmethod\n",
        "    def show_image(tensor: torch.Tensor, title: str = \"Image\", save_: bool = False, filename: str = None):\n",
        "        tensor = tensor.cpu().clone()\n",
        "        if len(tensor.shape) == 4:\n",
        "            tensor = tensor.squeeze(0)\n",
        "        elif len(tensor.shape) == 2:\n",
        "            tensor = tensor.unsqueeze(0)\n",
        "        elif len(tensor.shape) > 4 or len(tensor.shape) < 2:\n",
        "            raise ValueError(f\"Bad Input shape: {tensor.shape}\")\n",
        "\n",
        "        img = ToPILImage()(tensor)\n",
        "        plt.imshow(img)\n",
        "        plt.title(title)\n",
        "        plt.pause(0.001)\n",
        "\n",
        "        if save_:\n",
        "            img.save(fp=filename)\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, con_layers: list = ['conv4_2'], sty_layers: list = None,\n",
        "                 mean: list = [0.485, 0.456, 0.406], stdv: list = [0.229, 0.224, 0.225]):\n",
        "        super().__init__()\n",
        "\n",
        "        mapping_dict = {\"conv1_1\": 0, \"conv1_2\": 2,\n",
        "                        \"conv2_1\": 5, \"conv2_2\": 7,\n",
        "                        \"conv3_1\": 10, \"conv3_2\": 12, \"conv3_3\": 14, \"conv3_4\": 16,\n",
        "                        \"conv4_1\": 19, \"conv4_2\": 21, \"conv4_3\": 23, \"conv4_4\": 25,\n",
        "                        \"conv5_1\": 28, \"conv5_2\": 30, \"conv5_3\": 32, \"conv5_4\": 34}\n",
        "\n",
        "        mean = torch.tensor(mean, dtype=torch.float, device=device)\n",
        "        stdv = torch.tensor(stdv, dtype=torch.float, device=device)\n",
        "        self.transforms = T.Normalize(mean, stdv)\n",
        "\n",
        "        self.con_layers = [(mapping_dict[layer] + 1) for layer in con_layers]\n",
        "        self.sty_layers = [(mapping_dict[layer] + 1) for layer in sty_layers]\n",
        "\n",
        "        self.vgg19 = models.vgg19(pretrained=True).features\n",
        "        self.vgg19 = self.vgg19.to(device).eval()\n",
        "\n",
        "        for name, layer in self.vgg19.named_children():\n",
        "            if isinstance(layer, nn.MaxPool2d):\n",
        "                self.vgg19[int(name)] = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, tensor: torch.Tensor) -> dict:\n",
        "        sty_feat_maps = []\n",
        "        con_feat_maps = []\n",
        "        tensor = self.transforms(tensor)\n",
        "        x = tensor.unsqueeze(0)\n",
        "\n",
        "        for name, layer in self.vgg19.named_children():\n",
        "            x = layer(x)\n",
        "            if int(name) in self.con_layers:\n",
        "                con_feat_maps.append(x)\n",
        "            if int(name) in self.sty_layers:\n",
        "                sty_feat_maps.append(x)\n",
        "\n",
        "        return {\"Con_features\": con_feat_maps, \"Sty_features\": sty_feat_maps}\n",
        "\n",
        "class NeuralStyleTransfer:\n",
        "    def __init__(self, con_image: torch.Tensor, sty_image: torch.Tensor, size=512,\n",
        "                 con_layers: list = None, sty_layers: list = None,\n",
        "                 con_loss_wt: float = 1., sty_loss_wt: float = 1., var_loss_wt=1.):\n",
        "        self.con_loss_wt = con_loss_wt\n",
        "        self.sty_loss_wt = sty_loss_wt\n",
        "        self.var_loss_wt = var_loss_wt\n",
        "        self.size = size\n",
        "\n",
        "        self.model = MyModel(con_layers=con_layers, sty_layers=sty_layers)\n",
        "        self.sty_target = self.model(sty_image)[\"Sty_features\"]\n",
        "        self.con_target = self.model(con_image)[\"Con_features\"]\n",
        "\n",
        "        self.var_image = con_image.clone().requires_grad_(True).to(device)\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_var_loss(tensor: torch.Tensor) -> torch.Tensor:\n",
        "        return (torch.sum(torch.abs(tensor[:, :, :-1] - tensor[:, :, 1:])) +\n",
        "                torch.sum(torch.abs(tensor[:, :-1, :] - tensor[:, 1:, :])))\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_con_loss(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "        return 0.5 * torch.sum(torch.pow(pred - target, 2))\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_gram_matrix(tensor: torch.Tensor) -> torch.Tensor:\n",
        "        b, c, h, w = tensor.size()\n",
        "        tensor_ = tensor.view(b * c, h * w)\n",
        "        return torch.mm(tensor_, tensor_.t())\n",
        "\n",
        "    def _get_sty_loss(self, pred: torch.Tensor, target: torch.Tensor):\n",
        "        Z = np.power(np.prod(pred.size()), 2, dtype=np.float64)\n",
        "        pred = self._get_gram_matrix(pred)\n",
        "        return 0.25 * torch.sum(torch.pow(pred - target, 2)).div(Z)\n",
        "\n",
        "    def _get_tot_loss(self, output: torch.Tensor):\n",
        "        con_output = output[\"Con_features\"]\n",
        "        sty_output = output[\"Sty_features\"]\n",
        "\n",
        "        con_loss = [self._get_con_loss(con_output[i], self.con_target[i]) for i in range(len(con_output))]\n",
        "        sty_loss = [self._get_sty_loss(sty_output[i], self.sty_target[i]) for i in range(len(sty_output))]\n",
        "\n",
        "        con_loss = torch.mean(torch.stack(con_loss)) * self.con_loss_wt\n",
        "        sty_loss = torch.mean(torch.stack(sty_loss)) * self.sty_loss_wt\n",
        "        var_loss = self._get_var_loss(self.var_image) * self.var_loss_wt\n",
        "\n",
        "        return con_loss.to(device), sty_loss.to(device), var_loss.to(device)\n",
        "\n",
        "    def fit(self, nb_epochs: int = 1, nb_iters: int = 1000, lr: float = 1e-2, eps: float = 1e-8,\n",
        "            betas: tuple = (0.9, 0.999)) -> torch.Tensor:\n",
        "        self.sty_target = [self._get_gram_matrix(x).detach().to(device) for x in self.sty_target]\n",
        "        self.con_target = [x.detach() for x in self.con_target]\n",
        "\n",
        "        optimizer = optim.Adam([self.var_image], lr=lr, betas=betas, eps=eps)\n",
        "\n",
        "        for _ in range(nb_epochs):\n",
        "            for _ in range(nb_iters):\n",
        "                self.var_image.data.clamp_(0, 1)\n",
        "                optimizer.zero_grad()\n",
        "                output = self.model(self.var_image.to(device))\n",
        "\n",
        "                con_loss, sty_loss, var_loss = self._get_tot_loss(output)\n",
        "                tot_loss = con_loss + sty_loss + var_loss\n",
        "\n",
        "                tot_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        return self.var_image.data.clamp_(0, 1)\n",
        "\n",
        "def style_transfer(content_image_path, *style_image_paths):\n",
        "    img_loader = ImageLoader(size=(512, 512), resize=True)\n",
        "\n",
        "    con_image = img_loader.read_image(filepath=content_image_path)\n",
        "\n",
        "    con_layers = [\"conv4_2\"]\n",
        "    sty_layers = [\"conv1_1\", \"conv2_1\", \"conv3_1\", \"conv4_1\", \"conv5_1\"]\n",
        "\n",
        "    output_images = []\n",
        "\n",
        "    for style_image_path in style_image_paths:\n",
        "        sty_image = img_loader.read_image(filepath=style_image_path)\n",
        "\n",
        "        NST = NeuralStyleTransfer(con_image=con_image, sty_image=sty_image, size=(512, 512),\n",
        "                                  con_layers=con_layers, sty_layers=sty_layers,\n",
        "                                  con_loss_wt=1e-5, sty_loss_wt=1e4, var_loss_wt=5e-5)\n",
        "\n",
        "        output_image = NST.fit(nb_epochs=1, nb_iters=1000, lr=1e-2, eps=1e-8, betas=(0.9, 0.999))\n",
        "        output_images.append(output_image)\n",
        "\n",
        "        img_loader.show_image(output_image, save_=True, filename=f\"stylized_image_{len(output_images)}.jpg\")\n",
        "\n",
        "    if output_images:\n",
        "        segment_width = output_images[0].shape[2] // len(output_images)\n",
        "        combined_width = segment_width * len(output_images)\n",
        "        combined_image = Image.new('RGB', (combined_width, output_images[0].shape[1]))\n",
        "\n",
        "        for i, output_image in enumerate(output_images):\n",
        "            pil_image = ToPILImage()(output_image)\n",
        "            segment = pil_image.crop((i * segment_width, 0, (i + 1) * segment_width, pil_image.height))\n",
        "            combined_image.paste(segment, (i * segment_width, 0))\n",
        "\n",
        "        combined_image.save(\"combined_image.jpg\")\n",
        "\n",
        "    for i in range(len(output_images)):\n",
        "        pil_image = ToPILImage()(output_images[i])\n",
        "        pil_image.save(f\"stylized_image_{i + 1}.jpg\")\n",
        "\n",
        "    return [f\"stylized_image_{i + 1}.jpg\" for i in range(len(output_images))] + [\"combined_image.jpg\"]\n",
        "\n",
        "def launch_app():\n",
        "    n = int(input(\"Enter the number of style images: \"))\n",
        "    interface = gr.Interface(\n",
        "        fn=style_transfer,\n",
        "        inputs=[\"file\"] + [\"file\"] * n,\n",
        "        outputs=[\"image\"] * (n + 1),\n",
        "        title=\"Neural Style Transfer\",\n",
        "        description=\"Upload one content image and up to \" + str(n) + \" style images.\"\n",
        "    )\n",
        "    interface.launch()\n",
        "\n",
        "launch_app()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "r0_JCEVWyMJ8",
        "outputId": "7a5d613b-54af-422b-8c2f-e978bb0f50fc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of style images: 2\n",
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://c9deea0aad9fd4d4ec.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c9deea0aad9fd4d4ec.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}