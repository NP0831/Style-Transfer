{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exH1St-m6ydO"
      },
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn, torch.nn.functional as F,  torch.optim as optim\n",
        "import torchvision, torchvision.models as models, torchvision.transforms as T\n",
        "\n",
        "import numpy as np, pandas as pd, matplotlib as mpl, matplotlib.pyplot as plt\n",
        "import PIL.Image as Image, warnings; from IPython.display import clear_output\n",
        "\n",
        "mpl.rcParams[\"figure.figsize\"] = (14, 7); mpl.rcParams[\"axes.grid\"] = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "!pip install wget\n",
        "import os, wget, zipfile, shutil\n",
        "import warnings, itertools, functools\n",
        "\n",
        "from collections import OrderedDict\n",
        "from argparse import ArgumentParser\n",
        "from skimage import io as io, transform as tfm\n",
        "\n",
        "import matplotlib as mpl, matplotlib.pyplot as plt\n",
        "mpl.rcParams[\"figure.figsize\"] = (8, 4)\n",
        "mpl.rcParams[\"axes.grid\"     ] = False\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.nn import Conv2d as Conv, ConvTranspose2d as Deconv,  ReLU as Relu\n",
        "from torch.nn import InstanceNorm2d as InstanceNorm, BatchNorm2d as BatchNorm\n",
        "from torch.utils.tensorboard import SummaryWriter,  FileWriter,  RecordWriter\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
        "\n",
        "import torchvision\n",
        "import torchvision.utils as utils\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as models\n",
        "\n",
        "!pip install pytorch-lightning\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import loggers as pl_loggers\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, Callback, ModelCheckpoint# Import necessary libraries\n",
        "import torch, torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
        "import torchvision, torchvision.models as models, torchvision.transforms as T\n",
        "import numpy as np, pandas as pd, matplotlib as mpl, matplotlib.pyplot as plt\n",
        "import PIL.Image as Image, io\n",
        "from google.colab import files\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ao-CiygoVpIO",
        "outputId": "3b874a77-6f04-4e23-bbe4-18f7f2fde2d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=7c4df611ff6f2a85a54b2705041072fe08bc368576a762e17c762c9cadb9412d\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.5.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.5)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.6.1)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.5.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.11.8-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.10.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: numpy<2.0,>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.7.0->pytorch-lightning) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.2.0)\n",
            "Downloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.8-py3-none-any.whl (26 kB)\n",
            "Downloading torchmetrics-1.5.1-py3-none-any.whl (890 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.6/890.6 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.11.8 pytorch-lightning-2.4.0 torchmetrics-1.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o-QybMPWuXpD",
        "outputId": "3ba4a213-6f10-420e-efd4-7cdbfb83ee69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.4.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.3-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.4.2 (from gradio)\n",
            "  Downloading gradio_client-1.4.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting huggingface-hub>=0.25.1 (from gradio)\n",
            "  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart==0.0.12 (from gradio)\n",
            "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.7.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<1.0,>=0.1.1 (from gradio)\n",
            "  Downloading safehttpx-0.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.2-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.2->gradio) (2024.6.1)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.4.2->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.4.0-py3-none-any.whl (56.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.4.2-py3-none-any.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.8/319.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
            "Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.3-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.26.1-py3-none-any.whl (447 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.4/447.4 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading orjson-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruff-0.7.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.1-py3-none-any.whl (8.4 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.2-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, tomlkit, semantic-version, ruff, python-multipart, orjson, markupsafe, h11, ffmpy, aiofiles, uvicorn, starlette, huggingface-hub, httpcore, httpx, fastapi, safehttpx, gradio-client, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.24.7\n",
            "    Uninstalling huggingface-hub-0.24.7:\n",
            "      Successfully uninstalled huggingface-hub-0.24.7\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.3 ffmpy-0.4.0 gradio-5.4.0 gradio-client-1.4.2 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 huggingface-hub-0.26.1 markupsafe-2.1.5 orjson-3.10.10 pydub-0.25.1 python-multipart-0.0.12 ruff-0.7.1 safehttpx-0.1.1 semantic-version-2.10.0 starlette-0.41.2 tomlkit-0.12.0 uvicorn-0.32.0 websockets-12.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "huggingface_hub"
                ]
              },
              "id": "3155a980097740a3a7445df75bf6a99f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from PIL import Image, ImageEnhance\n",
        "import tempfile\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class ImageLoader:\n",
        "    def __init__(self, size: (int, tuple), resize: bool = True, interpolation=2):\n",
        "        transforms = []\n",
        "        if resize:\n",
        "            transforms.append(T.Resize(size=size, interpolation=interpolation))\n",
        "        transforms.append(T.ToTensor())\n",
        "        self.transforms = T.Compose(transforms)\n",
        "\n",
        "    def read_image(self, filepath: str) -> torch.Tensor:\n",
        "        image = Image.open(filepath)\n",
        "        image = self.transforms(image)\n",
        "        image = image.to(device, torch.float)\n",
        "        return image\n",
        "\n",
        "    @staticmethod\n",
        "    def show_image(tensor: torch.Tensor, title: str = \"Image\", save_: bool = False, filename: str = None):\n",
        "        tensor = tensor.cpu().clone()\n",
        "        if len(tensor.shape) == 4:\n",
        "            tensor = tensor.squeeze(0)\n",
        "        elif len(tensor.shape) == 2:\n",
        "            tensor = tensor.unsqueeze(0)\n",
        "        elif len(tensor.shape) > 4 or len(tensor.shape) < 2:\n",
        "            raise ValueError(f\"Bad Input shape: {tensor.shape}\")\n",
        "\n",
        "        transforms = T.ToPILImage()\n",
        "        img = transforms(tensor)\n",
        "        plt.imshow(img)\n",
        "        plt.title(title)\n",
        "        plt.pause(0.001)\n",
        "\n",
        "        if save_:\n",
        "            img.save(fp=filename)\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, con_layers: list = ['conv4_2'], sty_layers: list = None,\n",
        "                 mean: list = [0.485, 0.456, 0.406], stdv: list = [0.229, 0.224, 0.225]):\n",
        "        super().__init__()\n",
        "\n",
        "        mapping_dict = {\"conv1_1\": 0, \"conv1_2\": 2,\n",
        "                        \"conv2_1\": 5, \"conv2_2\": 7,\n",
        "                        \"conv3_1\": 10, \"conv3_2\": 12, \"conv3_3\": 14, \"conv3_4\": 16,\n",
        "                        \"conv4_1\": 19, \"conv4_2\": 21, \"conv4_3\": 23, \"conv4_4\": 25,\n",
        "                        \"conv5_1\": 28, \"conv5_2\": 30, \"conv5_3\": 32, \"conv5_4\": 34}\n",
        "\n",
        "        mean = torch.tensor(mean, dtype=torch.float, device=device)\n",
        "        stdv = torch.tensor(stdv, dtype=torch.float, device=device)\n",
        "        self.transforms = T.Normalize(mean, stdv)\n",
        "\n",
        "        self.con_layers = [(mapping_dict[layer] + 1) for layer in con_layers]\n",
        "        self.sty_layers = [(mapping_dict[layer] + 1) for layer in sty_layers]\n",
        "\n",
        "        self.vgg19 = models.vgg19(pretrained=True).features\n",
        "        self.vgg19 = self.vgg19.to(device).eval()\n",
        "\n",
        "        for name, layer in self.vgg19.named_children():\n",
        "            if isinstance(layer, nn.MaxPool2d):\n",
        "                self.vgg19[int(name)] = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, tensor: torch.Tensor) -> dict:\n",
        "        sty_feat_maps = []\n",
        "        con_feat_maps = []\n",
        "        tensor = self.transforms(tensor)\n",
        "        x = tensor.unsqueeze(0)\n",
        "\n",
        "        for name, layer in self.vgg19.named_children():\n",
        "            x = layer(x)\n",
        "            if int(name) in self.con_layers:\n",
        "                con_feat_maps.append(x)\n",
        "            if int(name) in self.sty_layers:\n",
        "                sty_feat_maps.append(x)\n",
        "\n",
        "        return {\"Con_features\": con_feat_maps, \"Sty_features\": sty_feat_maps}\n",
        "\n",
        "class NeuralStyleTransfer:\n",
        "    def __init__(self, con_image: torch.Tensor, sty_image: torch.Tensor, size=512,\n",
        "                 con_layers: list = None, sty_layers: list = None,\n",
        "                 con_loss_wt: float = 1., sty_loss_wt: float = 1., var_loss_wt=1.):\n",
        "        self.con_loss_wt = con_loss_wt\n",
        "        self.sty_loss_wt = sty_loss_wt\n",
        "        self.var_loss_wt = var_loss_wt\n",
        "        self.size = size\n",
        "\n",
        "        self.model = MyModel(con_layers=con_layers, sty_layers=sty_layers)\n",
        "        self.sty_target = self.model(sty_image)[\"Sty_features\"]\n",
        "        self.con_target = self.model(con_image)[\"Con_features\"]\n",
        "\n",
        "        self.var_image = con_image.clone().requires_grad_(True).to(device)\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_var_loss(tensor: torch.Tensor) -> torch.Tensor:\n",
        "        # Variation loss encourages spatial smoothness by penalizing large differences between adjacent pixels.\n",
        "        loss = (torch.sum(torch.abs(tensor[:, :, :-1] - tensor[:, :, 1:])) +\n",
        "                torch.sum(torch.abs(tensor[:, :-1, :] - tensor[:, 1:, :])))\n",
        "        return loss\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_con_loss(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "        return 0.5 * torch.sum(torch.pow(pred - target, 2))\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_gram_matrix(tensor: torch.Tensor) -> torch.Tensor:\n",
        "        b, c, h, w = tensor.size()\n",
        "        tensor_ = tensor.view(b * c, h * w)\n",
        "        gram_matrix = torch.mm(tensor_, tensor_.t())\n",
        "        return gram_matrix\n",
        "\n",
        "    def _get_sty_loss(self, pred: torch.Tensor, target: torch.Tensor):\n",
        "        Z = np.power(np.prod(pred.size()), 2, dtype=np.float64)\n",
        "        pred = self._get_gram_matrix(pred)\n",
        "        return 0.25 * torch.sum(torch.pow(pred - target, 2)).div(Z)\n",
        "\n",
        "    def _get_tot_loss(self, output: torch.Tensor):\n",
        "        con_output = output[\"Con_features\"]\n",
        "        nb_con_layers = len(con_output)\n",
        "        sty_output = output[\"Sty_features\"]\n",
        "        nb_sty_layers = len(sty_output)\n",
        "\n",
        "        con_loss = [self._get_con_loss(con_output[idx], self.con_target[idx]) for idx in range(nb_con_layers)]\n",
        "        sty_loss = [self._get_sty_loss(sty_output[idx], self.sty_target[idx]) for idx in range(nb_sty_layers)]\n",
        "\n",
        "        con_loss = torch.mean(torch.stack(con_loss)) * self.con_loss_wt\n",
        "        sty_loss = torch.mean(torch.stack(sty_loss)) * self.sty_loss_wt\n",
        "        var_loss = self._get_var_loss(self.var_image) * self.var_loss_wt\n",
        "\n",
        "        return con_loss.to(device), sty_loss.to(device), var_loss.to(device)\n",
        "\n",
        "    def fit(self, nb_epochs: int = 1, nb_iters: int = 1000, lr: float = 1e-2, eps: float = 1e-8,\n",
        "            betas: tuple = (0.9, 0.999)) -> torch.Tensor:\n",
        "        self.sty_target = [self._get_gram_matrix(x).detach().to(device) for x in self.sty_target]\n",
        "        self.con_target = [x.detach() for x in self.con_target]\n",
        "\n",
        "        optimizer = optim.Adam([self.var_image], lr=lr, betas=betas, eps=eps)\n",
        "\n",
        "        for epoch in range(nb_epochs):\n",
        "            for iter_ in range(nb_iters):\n",
        "                self.var_image.data.clamp_(0, 1)\n",
        "                optimizer.zero_grad()\n",
        "                output = self.model(self.var_image.to(device))\n",
        "\n",
        "                con_loss, sty_loss, var_loss = self._get_tot_loss(output)\n",
        "                tot_loss = con_loss + sty_loss + var_loss\n",
        "\n",
        "                tot_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        return self.var_image.data.clamp_(0, 1)\n",
        "\n",
        "def style_transfer(content_image_path, *style_image_paths):\n",
        "    img_loader = ImageLoader(size=(512, 512), resize=True)\n",
        "\n",
        "    # Load content image\n",
        "    con_image = img_loader.read_image(filepath=content_image_path)\n",
        "\n",
        "    con_layers = [\"conv4_2\"]\n",
        "    sty_layers = [\"conv1_1\", \"conv2_1\", \"conv3_1\", \"conv4_1\", \"conv5_1\"]\n",
        "\n",
        "    output_images = []\n",
        "\n",
        "    # Loop through each style image path\n",
        "    for style_image_path in style_image_paths:\n",
        "        sty_image = img_loader.read_image(filepath=style_image_path)\n",
        "\n",
        "        # Perform style transfer\n",
        "        NST = NeuralStyleTransfer(con_image=con_image, sty_image=sty_image, size=(512, 512),\n",
        "                                  con_layers=con_layers, sty_layers=sty_layers,\n",
        "                                  con_loss_wt=1e-5, sty_loss_wt=1e4, var_loss_wt=5e-5)\n",
        "\n",
        "        output_image = NST.fit(nb_epochs=1, nb_iters=1000, lr=1e-2, eps=1e-8, betas=(0.9, 0.999))\n",
        "        output_images.append(output_image)\n",
        "\n",
        "        # Show and save each stylized image\n",
        "        img_loader.show_image(output_image, save_=True, filename=f\"stylized_image_{len(output_images)}.jpg\")\n",
        "\n",
        "    # Combine output images into segments\n",
        "    num_segments = len(output_images)  # Number of images as the number of segments\n",
        "    if num_segments > 0:\n",
        "        segment_width = output_images[0].shape[2] // num_segments  # Height for each segment\n",
        "        combined_width = segment_width * num_segments\n",
        "\n",
        "        combined_image = Image.new('RGB', (combined_width, output_images[0].shape[1]))  # Width of one image, height of combined segments\n",
        "\n",
        "        for i, output_image in enumerate(output_images):\n",
        "            pil_image = T.ToPILImage()(output_image)  # Convert output tensor to PIL\n",
        "            segment = pil_image.crop((i * segment_width, 0,(i + 1) * segment_width, pil_image.height))  # Crop segment\n",
        "            combined_image.paste(segment, (i * segment_width, 0))  # Paste segment into combined image\n",
        "\n",
        "        combined_image.save(\"combined_image.jpg\")\n",
        "\n",
        "    # Save all individual stylized images\n",
        "    for i in range(len(output_images)):\n",
        "        pil_image = T.ToPILImage()(output_images[i])\n",
        "        pil_image.save(f\"stylized_image_{i + 1}.jpg\")\n",
        "\n",
        "    return [f\"stylized_image_{i + 1}.jpg\" for i in range(len(output_images))] + [\"combined_image.jpg\"]\n",
        "\n",
        "def launch_app():\n",
        "    # User input for number of images and segmentation type\n",
        "    n = int(input(\"Enter the number of style images: \"))\n",
        "\n",
        "    # Create Gradio interface\n",
        "    interface = gr.Interface(\n",
        "        fn=style_transfer,\n",
        "        inputs=[\"file\"] + [\"file\"] * n,  # Allow for one content image and up to n style images\n",
        "        outputs=[\"image\"] * (n + 1),  # Outputs for n style images + 1 combined image\n",
        "        title=\"Neural Style Transfer\",\n",
        "        description=\"Upload one content image and up to \" + str(n) + \" style images.\"\n",
        "    )\n",
        "\n",
        "    # Launch the Gradio app\n",
        "    interface.launch()\n",
        "\n",
        "# Run the app\n",
        "launch_app()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "z9lTNzWsuO68",
        "outputId": "ad3e7533-509e-498a-91e3-d634365f6d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of style images: 2\n",
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://92a16b39cb2e8cbecc.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://92a16b39cb2e8cbecc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}